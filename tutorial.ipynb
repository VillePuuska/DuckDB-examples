{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Getting started\n",
    "\n",
    "## First commands\n",
    "\n",
    "Getting DuckDB running is as simple as pip installing the package `duckdb` and importing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't need to persist the database after you're done with your session, you can immediately run queries against the database with `duckdb.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'duckdb.duckdb.DuckDBPyRelation'>\n",
      "┌────────────────┐\n",
      "│ 'Hello World!' │\n",
      "│    varchar     │\n",
      "├────────────────┤\n",
      "│ Hello World!   │\n",
      "└────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT 'Hello World!'\n",
    "\"\"\"\n",
    "res = duckdb.sql(query)\n",
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly print the result to see the result in the above format. Alternatively, the result object has a method .show() you can use instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────┐\n",
      "│ 'Hello World!' │\n",
      "│    varchar     │\n",
      "├────────────────┤\n",
      "│ Hello World!   │\n",
      "└────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If and when you need to access the query results with Python, you can convert the result for example to\n",
    "- Python object with `res.fetchall()`\n",
    "- a Pandas DataFrame with `res.df()` or `res.to_df()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello World!',)]\n"
     ]
    }
   ],
   "source": [
    "ls = res.fetchall()\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'Hello World!'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello World!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  'Hello World!'\n",
       "0   Hello World!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = res.df()\n",
    "print(type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can naturally create tables, insert values, create views and so on like in any database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┐\n",
       "│     name      │\n",
       "│    varchar    │\n",
       "├───────────────┤\n",
       "│ another_table │\n",
       "│ test_table    │\n",
       "└───────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE test_table (\n",
    "    int_col INTEGER,\n",
    "    str_col VARCHAR\n",
    ");\n",
    "CREATE OR REPLACE TABLE another_table (\n",
    "    int_col INTEGER\n",
    ")\n",
    "\"\"\"\n",
    "duckdb.sql(query)\n",
    "duckdb.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬─────────┐\n",
       "│ int_col │ str_col │\n",
       "│  int32  │ varchar │\n",
       "├─────────┼─────────┤\n",
       "│       1 │ Hello   │\n",
       "│       2 │         │\n",
       "│       3 │ World   │\n",
       "└─────────┴─────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "INSERT INTO test_table (int_col, str_col) VALUES\n",
    "    (1, 'Hello'),\n",
    "    (3, 'World'),\n",
    "    (2, ' ')\n",
    "\"\"\"\n",
    "duckdb.sql(query)\n",
    "duckdb.sql(\"FROM test_table ORDER BY int_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬─────────┐\n",
       "│ int_col │ str_col │\n",
       "│  int32  │ varchar │\n",
       "├─────────┼─────────┤\n",
       "│       1 │ Hello   │\n",
       "│       3 │ World   │\n",
       "└─────────┴─────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE VIEW test_view AS (\n",
    "    FROM test_table\n",
    "    WHERE str_col != ' '\n",
    ")\n",
    "\"\"\"\n",
    "duckdb.sql(query)\n",
    "duckdb.sql(\"FROM test_view ORDER BY int_col\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the DuckDB SQL dialect you can omit `SELECT *`. You can also\n",
    "- reorder `SELECT` and `FROM`, i.e. you can query `FROM table SELECT cols`,\n",
    "- exclude columns instead of listing all of the columns you want, i.e. `SELECT * EXCLUDE(cols, we, do, not, want) FROM table`,\n",
    "- group by all non-aggregated columns, i.e. `SELECT ... FROM table GROUP BY ALL`.\n",
    "\n",
    "See the [DuckDB documentation](https://duckdb.org/docs/sql/introduction) for the SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the database\n",
    "\n",
    "If you want to persist your database in a file or open a previously saved database, you first need to create a connection to it and the use the connection instead of `duckdb` to run your queries. For example, the following cell either creates a new database to a file test.db or if the file already exists the cell loads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────┐\n",
      "│ 'Hello World!' │\n",
      "│    varchar     │\n",
      "├────────────────┤\n",
      "│ Hello World!   │\n",
      "└────────────────┘\n",
      "\n",
      "┌───────────────────┐\n",
      "│       name        │\n",
      "│      varchar      │\n",
      "├───────────────────┤\n",
      "│ yet_another_table │\n",
      "└───────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conn = duckdb.connect(\"test.db\")\n",
    "query = \"\"\"\n",
    "SELECT 'Hello World!'\n",
    "\"\"\"\n",
    "conn.sql(query).show()\n",
    "\n",
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE yet_another_table (\n",
    "    col INTEGER\n",
    ")\n",
    "\"\"\"\n",
    "conn.sql(query)\n",
    "conn.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "[Extensions](https://duckdb.org/docs/extensions/overview.html) allow you to add functionality to DuckDB. To see the list of extensions, you can use the `duckdb_extensions()` SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────┬─────────┬───────────┬──────────────────────┬──────────────────────────────────┬───────────────────┐\n",
       "│  extension_name  │ loaded  │ installed │     install_path     │           description            │      aliases      │\n",
       "│     varchar      │ boolean │  boolean  │       varchar        │             varchar              │     varchar[]     │\n",
       "├──────────────────┼─────────┼───────────┼──────────────────────┼──────────────────────────────────┼───────────────────┤\n",
       "│ arrow            │ false   │ false     │                      │ A zero-copy data integration b…  │ []                │\n",
       "│ autocomplete     │ false   │ false     │                      │ Adds support for autocomplete …  │ []                │\n",
       "│ aws              │ false   │ false     │                      │ Provides features that depend …  │ []                │\n",
       "│ azure            │ false   │ false     │                      │ Adds a filesystem abstraction …  │ []                │\n",
       "│ excel            │ false   │ false     │                      │ Adds support for Excel-like fo…  │ []                │\n",
       "│ fts              │ false   │ true      │ (BUILT-IN)           │ Adds support for Full-Text Sea…  │ []                │\n",
       "│ httpfs           │ false   │ false     │                      │ Adds support for reading and w…  │ [http, https, s3] │\n",
       "│ iceberg          │ false   │ false     │                      │ Adds support for Apache Iceberg  │ []                │\n",
       "│ icu              │ true    │ true      │ (BUILT-IN)           │ Adds support for time zones an…  │ []                │\n",
       "│ inet             │ false   │ false     │                      │ Adds support for IP-related da…  │ []                │\n",
       "│ jemalloc         │ true    │ true      │ (BUILT-IN)           │ Overwrites system allocator wi…  │ []                │\n",
       "│ json             │ true    │ true      │ (BUILT-IN)           │ Adds support for JSON operations │ []                │\n",
       "│ motherduck       │ false   │ false     │                      │ Enables motherduck integration…  │ [md]              │\n",
       "│ mysql_scanner    │ false   │ false     │                      │ Adds support for connecting to…  │ [mysql]           │\n",
       "│ parquet          │ true    │ true      │ (BUILT-IN)           │ Adds support for reading and w…  │ []                │\n",
       "│ postgres_scanner │ false   │ true      │ /home/codespace/.d…  │ Adds support for connecting to…  │ [postgres]        │\n",
       "│ spatial          │ false   │ false     │                      │ Geospatial extension that adds…  │ []                │\n",
       "│ sqlite_scanner   │ false   │ false     │                      │ Adds support for reading and w…  │ [sqlite, sqlite3] │\n",
       "│ substrait        │ false   │ false     │                      │ Adds support for the Substrait…  │ []                │\n",
       "│ tpcds            │ true    │ true      │ (BUILT-IN)           │ Adds TPC-DS data generation an…  │ []                │\n",
       "│ tpch             │ true    │ true      │ (BUILT-IN)           │ Adds TPC-H data generation and…  │ []                │\n",
       "│ visualizer       │ false   │ false     │                      │ Creates an HTML-based visualiz…  │ []                │\n",
       "├──────────────────┴─────────┴───────────┴──────────────────────┴──────────────────────────────────┴───────────────────┤\n",
       "│ 22 rows                                                                                                    6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"FROM duckdb_extensions()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will need the `postgres` extension, or `postgres_scanner` more specifically. If the extension is listed as not installed, let's install and load it now since we'll use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSTALL postgres\")\n",
    "duckdb.sql(\"LOAD postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────┬─────────┬───────────┬──────────────────────┬──────────────────────────────────┬───────────────────┐\n",
       "│  extension_name  │ loaded  │ installed │     install_path     │           description            │      aliases      │\n",
       "│     varchar      │ boolean │  boolean  │       varchar        │             varchar              │     varchar[]     │\n",
       "├──────────────────┼─────────┼───────────┼──────────────────────┼──────────────────────────────────┼───────────────────┤\n",
       "│ arrow            │ false   │ false     │                      │ A zero-copy data integration b…  │ []                │\n",
       "│ autocomplete     │ false   │ false     │                      │ Adds support for autocomplete …  │ []                │\n",
       "│ aws              │ false   │ false     │                      │ Provides features that depend …  │ []                │\n",
       "│ azure            │ false   │ false     │                      │ Adds a filesystem abstraction …  │ []                │\n",
       "│ excel            │ false   │ false     │                      │ Adds support for Excel-like fo…  │ []                │\n",
       "│ fts              │ false   │ true      │ (BUILT-IN)           │ Adds support for Full-Text Sea…  │ []                │\n",
       "│ httpfs           │ false   │ false     │                      │ Adds support for reading and w…  │ [http, https, s3] │\n",
       "│ iceberg          │ false   │ false     │                      │ Adds support for Apache Iceberg  │ []                │\n",
       "│ icu              │ true    │ true      │ (BUILT-IN)           │ Adds support for time zones an…  │ []                │\n",
       "│ inet             │ false   │ false     │                      │ Adds support for IP-related da…  │ []                │\n",
       "│ jemalloc         │ true    │ true      │ (BUILT-IN)           │ Overwrites system allocator wi…  │ []                │\n",
       "│ json             │ true    │ true      │ (BUILT-IN)           │ Adds support for JSON operations │ []                │\n",
       "│ motherduck       │ false   │ false     │                      │ Enables motherduck integration…  │ [md]              │\n",
       "│ mysql_scanner    │ false   │ false     │                      │ Adds support for connecting to…  │ [mysql]           │\n",
       "│ parquet          │ true    │ true      │ (BUILT-IN)           │ Adds support for reading and w…  │ []                │\n",
       "│ postgres_scanner │ true    │ true      │ /home/codespace/.d…  │ Adds support for connecting to…  │ [postgres]        │\n",
       "│ spatial          │ false   │ false     │                      │ Geospatial extension that adds…  │ []                │\n",
       "│ sqlite_scanner   │ false   │ false     │                      │ Adds support for reading and w…  │ [sqlite, sqlite3] │\n",
       "│ substrait        │ false   │ false     │                      │ Adds support for the Substrait…  │ []                │\n",
       "│ tpcds            │ true    │ true      │ (BUILT-IN)           │ Adds TPC-DS data generation an…  │ []                │\n",
       "│ tpch             │ true    │ true      │ (BUILT-IN)           │ Adds TPC-H data generation and…  │ []                │\n",
       "│ visualizer       │ false   │ false     │                      │ Creates an HTML-based visualiz…  │ []                │\n",
       "├──────────────────┴─────────┴───────────┴──────────────────────┴──────────────────────────────────┴───────────────────┤\n",
       "│ 22 rows                                                                                                    6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"FROM duckdb_extensions()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also install and load extensions with the Python API functions `duckdb.install_extension` and `duckdb.load_extension`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Dataframes\n",
    "\n",
    "DuckDB can interact with Pandas and Polars dataframes in both ways, i.e. it can read dataframes and can convert query results to dataframes. See the documentation for full details: [Pandas](https://duckdb.org/docs/archive/0.9.2/guides/python/import_pandas), [Polars](https://duckdb.org/docs/archive/0.9.2/guides/python/polars).\n",
    "\n",
    "As an example, we'll load one of our generated example parquet-files with all three -- DuckDB, Pandas, and Polars -- and see how to convert between the three. If you haven't already, run the script `generate_example_data.py` before moving on. Note that to work with Pandas dataframes you only need to pip install Pandas, but to work with Polars dataframes you also need to install pyarrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at querying Pandas dataframes with duckdb. If you have named the dataframe e.g. `df_pandas`, you can refer to it in a SQL query in DuckDB just like you'd refer to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬────────────────────┬────────────────────┬────────────────────┬──────────────┐\n",
       "│  id   │      timestamp      │        col3        │        col1        │        col2        │     tags     │\n",
       "│ int64 │       varchar       │       double       │       double       │       double       │  varchar[]   │\n",
       "├───────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼──────────────┤\n",
       "│   101 │ 2023-12-13T06:32:49 │               NULL │               NULL │               NULL │ NULL         │\n",
       "│   102 │ 2023-04-19T09:54:18 │  329.3245699179571 │  123.8749330113198 │ 208.80899087038708 │ [d, a, c, b] │\n",
       "│   103 │ 2023-08-21T06:28:57 │               NULL │               NULL │               NULL │ [a, b, c, d] │\n",
       "│   104 │ 2023-03-02T16:37:20 │  353.0100537270686 │  158.4221395570704 │  293.8240259081357 │ NULL         │\n",
       "│   105 │ 2023-03-13T15:54:04 │  308.3782509448798 │               NULL │ 228.87403284948138 │ [d]          │\n",
       "│   106 │ 2023-04-08T02:23:11 │               NULL │               NULL │               NULL │ [a, d]       │\n",
       "│   107 │ 2023-06-26T11:38:33 │               NULL │               NULL │               NULL │ [d, c, b, a] │\n",
       "│   108 │ 2023-07-15T02:20:44 │               NULL │               NULL │               NULL │ NULL         │\n",
       "│   109 │ 2023-03-11T05:21:01 │ 317.10323785389295 │               NULL │ 297.34352553451197 │ NULL         │\n",
       "│   110 │ 2023-03-11T19:56:56 │ 370.60898161974205 │ 129.09176486006268 │ 232.18750275776392 │ [b, d]       │\n",
       "│   111 │ 2023-05-30T14:55:40 │               NULL │               NULL │               NULL │ [d, c, b, a] │\n",
       "│   112 │ 2023-02-18T05:50:05 │               NULL │ 112.07713977761134 │               NULL │ NULL         │\n",
       "│   113 │ 2023-04-22T20:30:05 │  335.8108565849494 │ 151.44172843921754 │ 233.00778091140404 │ NULL         │\n",
       "│   114 │ 2023-02-27T00:35:55 │ 398.52827883313086 │               NULL │ 257.70437830794714 │ [a, d, b]    │\n",
       "│   115 │ 2023-06-09T19:58:53 │               NULL │ 147.03900910947416 │               NULL │ [b, a, d]    │\n",
       "│   116 │ 2023-08-09T05:08:58 │               NULL │               NULL │ 299.15795076704137 │ [a, c, d, b] │\n",
       "│   117 │ 2023-09-13T20:01:16 │               NULL │               NULL │               NULL │ [c, d, b]    │\n",
       "│   118 │ 2023-12-10T19:56:21 │               NULL │               NULL │ 225.55080482819517 │ [b, d, a, c] │\n",
       "│   119 │ 2023-07-08T20:10:14 │  382.1607708365713 │ 121.60396933418612 │               NULL │ [a, d, c]    │\n",
       "│   120 │ 2023-05-30T08:39:34 │  349.2522704806417 │               NULL │               NULL │ [d, c]       │\n",
       "├───────┴─────────────────────┴────────────────────┴────────────────────┴────────────────────┴──────────────┤\n",
       "│ 20 rows                                                                                         6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_parquet(\"data/df_1.parquet\")\n",
    "\n",
    "duckdb.sql(\"FROM df_pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already saw in the previous section, we can call `.df()` or `.to_df()` in a DuckDB query result to convert it to a Pandas dataframe. For example, let's run a query to get all tags from the dataframe and convert it back to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag\n",
       "0   a\n",
       "1   b\n",
       "2   c\n",
       "3   d"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unnest explodes the lists in the tags-column so DISTINCT(unnest(tags)) gets all the tags that appear in the column\n",
    "duckdb.sql(\"SELECT DISTINCT(unnest(tags)) AS tag FROM df_pandas ORDER BY ALL\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then see how to do the same steps with Polars instead. The only actual difference when working with Polars dataframes is that a duckdb query result is converted to a Polars dataframe with the `.pl()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬────────────────────┬────────────────────┬────────────────────┬──────────────┐\n",
       "│  id   │      timestamp      │        col3        │        col1        │        col2        │     tags     │\n",
       "│ int64 │       varchar       │       double       │       double       │       double       │  varchar[]   │\n",
       "├───────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼──────────────┤\n",
       "│   101 │ 2023-12-13T06:32:49 │               NULL │               NULL │               NULL │ NULL         │\n",
       "│   102 │ 2023-04-19T09:54:18 │  329.3245699179571 │  123.8749330113198 │ 208.80899087038708 │ [d, a, c, b] │\n",
       "│   103 │ 2023-08-21T06:28:57 │               NULL │               NULL │               NULL │ [a, b, c, d] │\n",
       "│   104 │ 2023-03-02T16:37:20 │  353.0100537270686 │  158.4221395570704 │  293.8240259081357 │ NULL         │\n",
       "│   105 │ 2023-03-13T15:54:04 │  308.3782509448798 │               NULL │ 228.87403284948138 │ [d]          │\n",
       "│   106 │ 2023-04-08T02:23:11 │               NULL │               NULL │               NULL │ [a, d]       │\n",
       "│   107 │ 2023-06-26T11:38:33 │               NULL │               NULL │               NULL │ [d, c, b, a] │\n",
       "│   108 │ 2023-07-15T02:20:44 │               NULL │               NULL │               NULL │ NULL         │\n",
       "│   109 │ 2023-03-11T05:21:01 │ 317.10323785389295 │               NULL │ 297.34352553451197 │ NULL         │\n",
       "│   110 │ 2023-03-11T19:56:56 │ 370.60898161974205 │ 129.09176486006268 │ 232.18750275776392 │ [b, d]       │\n",
       "│   111 │ 2023-05-30T14:55:40 │               NULL │               NULL │               NULL │ [d, c, b, a] │\n",
       "│   112 │ 2023-02-18T05:50:05 │               NULL │ 112.07713977761134 │               NULL │ NULL         │\n",
       "│   113 │ 2023-04-22T20:30:05 │  335.8108565849494 │ 151.44172843921754 │ 233.00778091140404 │ NULL         │\n",
       "│   114 │ 2023-02-27T00:35:55 │ 398.52827883313086 │               NULL │ 257.70437830794714 │ [a, d, b]    │\n",
       "│   115 │ 2023-06-09T19:58:53 │               NULL │ 147.03900910947416 │               NULL │ [b, a, d]    │\n",
       "│   116 │ 2023-08-09T05:08:58 │               NULL │               NULL │ 299.15795076704137 │ [a, c, d, b] │\n",
       "│   117 │ 2023-09-13T20:01:16 │               NULL │               NULL │               NULL │ [c, d, b]    │\n",
       "│   118 │ 2023-12-10T19:56:21 │               NULL │               NULL │ 225.55080482819517 │ [b, d, a, c] │\n",
       "│   119 │ 2023-07-08T20:10:14 │  382.1607708365713 │ 121.60396933418612 │               NULL │ [a, d, c]    │\n",
       "│   120 │ 2023-05-30T08:39:34 │  349.2522704806417 │               NULL │               NULL │ [d, c]       │\n",
       "├───────┴─────────────────────┴────────────────────┴────────────────────┴────────────────────┴──────────────┤\n",
       "│ 20 rows                                                                                         6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polars = pl.read_parquet(\"data/df_1.parquet\")\n",
    "\n",
    "duckdb.sql(\"FROM df_polars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>tag</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;a&quot;</td></tr><tr><td>&quot;b&quot;</td></tr><tr><td>&quot;c&quot;</td></tr><tr><td>&quot;d&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 1)\n",
       "┌─────┐\n",
       "│ tag │\n",
       "│ --- │\n",
       "│ str │\n",
       "╞═════╡\n",
       "│ a   │\n",
       "│ b   │\n",
       "│ c   │\n",
       "│ d   │\n",
       "└─────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"SELECT DISTINCT(unnest(tags)) AS tag FROM df_pandas ORDER BY ALL\").pl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Working with files\n",
    "\n",
    "DuckDB can read and write CSV, JSON, and Parquet files out of the box. You can add more filetypes with extensions. We'll look at examples of Parquet and JSON files. See the [documentation](https://duckdb.org/docs/archive/0.9.2/data/overview) for more information.\n",
    "\n",
    "## Reading files\n",
    "\n",
    "Reading from files can be as simple as querying the file as if it was a table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬─────────────────────┬──────────────┐\n",
      "│  id   │      timestamp      │     tags     │\n",
      "│ int64 │       varchar       │  varchar[]   │\n",
      "├───────┼─────────────────────┼──────────────┤\n",
      "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │\n",
      "└───────┴─────────────────────┴──────────────┘\n",
      "\n",
      "┌───────┬─────────────────────┬───────────────────┬───────────────────┬────────────────────┬──────────────┐\n",
      "│  id   │      timestamp      │       col3        │       col1        │        col2        │     tags     │\n",
      "│ int64 │       varchar       │      double       │      double       │       double       │  varchar[]   │\n",
      "├───────┼─────────────────────┼───────────────────┼───────────────────┼────────────────────┼──────────────┤\n",
      "│   101 │ 2023-12-13T06:32:49 │              NULL │              NULL │               NULL │ NULL         │\n",
      "│   102 │ 2023-04-19T09:54:18 │ 329.3245699179571 │ 123.8749330113198 │ 208.80899087038708 │ [d, a, c, b] │\n",
      "│   103 │ 2023-08-21T06:28:57 │              NULL │              NULL │               NULL │ [a, b, c, d] │\n",
      "│   104 │ 2023-03-02T16:37:20 │ 353.0100537270686 │ 158.4221395570704 │  293.8240259081357 │ NULL         │\n",
      "│   105 │ 2023-03-13T15:54:04 │ 308.3782509448798 │              NULL │ 228.87403284948138 │ [d]          │\n",
      "└───────┴─────────────────────┴───────────────────┴───────────────────┴────────────────────┴──────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duckdb.sql(\"FROM 'data/1_record.json'\").show()\n",
    "duckdb.sql(\"FROM 'data/df_1.parquet' LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also have the functions `read_csv_auto()`, `read_csv()`, `read_parquet()`, `read_json()` and `read_json_auto()` if you need more control or you need to set some parameters.\n",
    "\n",
    "DuckDB also supports reading multiple files by providing a list of filenames or by globbing. The files can even be of different filetypes. For example, if we want to read all of the Parquet-files in our `data/` directory, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────────┬────────────────────┬──────────────────┐\n",
      "│     mean(col1)     │     mean(col2)     │    mean(col3)    │\n",
      "│       double       │       double       │      double      │\n",
      "├────────────────────┼────────────────────┼──────────────────┤\n",
      "│ 149.85944914296797 │ 245.98588768634744 │ 350.500765219307 │\n",
      "└────────────────────┴────────────────────┴──────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duckdb.sql(\"FROM read_parquet('data/*.parquet', union_by_name = true) SELECT MEAN(col1), MEAN(col2), MEAN(col3)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the function `read_parquet()` with the parameter `union_by_name = true`. This tells DuckDB to combine the schemas of the files by name instead of position which is the default behavior. We need to do this since our randomly generated Parquet-files are not guaranteed to have the columns in the same order!\n",
    "\n",
    "If we want to read only the files `1_record.json` and `2_record.json` we can give the filenames as a list like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬─────────────────────┬──────────────┬───────────────────┬────────────────────┬────────────────────┐\n",
      "│  id   │      timestamp      │     tags     │       col1        │        col3        │        col2        │\n",
      "│ int64 │       varchar       │  varchar[]   │      double       │       double       │       double       │\n",
      "├───────┼─────────────────────┼──────────────┼───────────────────┼────────────────────┼────────────────────┤\n",
      "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │              NULL │               NULL │               NULL │\n",
      "│     2 │ 2023-11-11T03:38:00 │ [d]          │ 143.2773325891486 │ 348.14722168023326 │ 241.33740756266872 │\n",
      "└───────┴─────────────────────┴──────────────┴───────────────────┴────────────────────┴────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duckdb.sql(\"FROM read_json_auto(['data/1_record.json', 'data/2_record.json'], union_by_name = true)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead of querying a file you want to read a file and write the data to a table in your database, you can use the [COPY ... FROM](https://duckdb.org/docs/sql/statements/copy#copy--from) statement. Two important caveats to note with COPY ... FROM:\n",
    "- the table must already exists so you cannot create a new table with COPY ... FROM,\n",
    "- the columns and their order must match between the table and file, i.e. you cannot match columns by their names.\n",
    "\n",
    "If you need to create a new table based on a file or if you need to infer the column order, COPY ... FROM is not enough on its own; you will need to use e.g. CREATE TABLE AS SELECT or INSERT INTO ... BY NAME statements. Namely, in our example where our files are generated randomly and the column orders are not fixed, we can not simply COPY ... FROM any of our files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing files\n",
    "\n",
    "Writing files is even simpler. You do it with the [COPY ... TO](https://duckdb.org/docs/sql/statements/copy#copy--to) statement. For example, if we'd like to read all of the records in the JSON-files, exclude the tags column, and write the result to a CSV-file, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "COPY (\n",
    "    FROM read_json_auto('data/*.json', union_by_name = true)\n",
    "    SELECT * EXCLUDE(tags)\n",
    ")\n",
    "TO 'data/json-records.csv'\n",
    "\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [documentation](https://duckdb.org/docs/sql/statements/copy#copy--to) for all the format options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Interacting with databases\n",
    "\n",
    "DuckDB has extensions for [MySQL](https://duckdb.org/docs/extensions/mysql), [PostgreSQL](https://duckdb.org/docs/extensions/postgres.html), and [SQLite](https://duckdb.org/docs/extensions/sqlite). These extensions allow you to insert, query, update, and delete data and tables in said databases directly from DuckDB. We'll use Postgres as an example, but MySQL and SQLite work similarly. See the documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTACH and USE\n",
    "\n",
    "First, we need to connect to a database. This is done with the ATTACH command. Assuming that you have a PostgreSQL running at localhost, with a database named `postgres` and a user `postgres` with the password `postgres`, you attach to it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"ATTACH 'dbname=postgres user=postgres password=postgres host=localhost' AS pg (TYPE postgres)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the connection can be given as a libpq connection string or as a PostgreSQL URI. The parameters can also be read from environment variables. See the [documentation](https://duckdb.org/docs/extensions/postgres.html#connecting).\n",
    "\n",
    "Now, if we want to use the SHOW TABLES command to list all tables in the Postgres database, we need to make the database as the default database for DuckDB. You can do it with the USE command. We can also specify a schema with USE. For example, let's create a new schema named duckdbexamples in the Postgres database and switch to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────┐\n",
      "│  name   │\n",
      "│ varchar │\n",
      "├─────────┤\n",
      "│ 0 rows  │\n",
      "└─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duckdb.sql(\"CREATE SCHEMA pg.duckdbexamples\")\n",
    "duckdb.sql(\"USE pg.duckdbexamples\")\n",
    "duckdb.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying\n",
    "\n",
    "Now that we have attached the Postgres database, we can run queries against it just like we would against a DuckDB database. For example we can create a table in the schema just like we would in the DuckDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬──────────────┬───────────────────┬────────────────────┬────────────────────┐\n",
       "│  id   │      timestamp      │     tags     │       col1        │        col3        │        col2        │\n",
       "│ int64 │       varchar       │  varchar[]   │      double       │       double       │       double       │\n",
       "├───────┼─────────────────────┼──────────────┼───────────────────┼────────────────────┼────────────────────┤\n",
       "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │              NULL │               NULL │               NULL │\n",
       "│     2 │ 2023-11-11T03:38:00 │ [d]          │ 143.2773325891486 │ 348.14722168023326 │ 241.33740756266872 │\n",
       "└───────┴─────────────────────┴──────────────┴───────────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CREATE OR REPLACE TABLE pg.duckdbexamples.pg_test_table AS FROM read_json_auto(['data/1_record.json', 'data/2_record.json'], union_by_name = true);\n",
    "FROM pg.duckdbexamples.pg_test_table;\n",
    "\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────┐\n",
       "│     name      │\n",
       "│    varchar    │\n",
       "├───────────────┤\n",
       "│ pg_test_table │\n",
       "└───────────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we switch back to the in-memory DuckDB database, SHOW TABLES lists the tables we created in DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────┐\n",
      "│     name      │\n",
      "│    varchar    │\n",
      "├───────────────┤\n",
      "│ another_table │\n",
      "│ test_table    │\n",
      "│ test_view     │\n",
      "└───────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duckdb.sql(\"USE memory\")\n",
    "duckdb.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still of course query the Postgres table without needing to USE pg.duckdbexamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬──────────────┬───────────────────┬────────────────────┬────────────────────┐\n",
       "│  id   │      timestamp      │     tags     │       col1        │        col3        │        col2        │\n",
       "│ int64 │       varchar       │  varchar[]   │      double       │       double       │       double       │\n",
       "├───────┼─────────────────────┼──────────────┼───────────────────┼────────────────────┼────────────────────┤\n",
       "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │              NULL │               NULL │               NULL │\n",
       "│     2 │ 2023-11-11T03:38:00 │ [d]          │ 143.2773325891486 │ 348.14722168023326 │ 241.33740756266872 │\n",
       "└───────┴─────────────────────┴──────────────┴───────────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"FROM pg.duckdbexamples.pg_test_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note especially that we are not using the PostgreSQL dialect. We are using DuckDB's SQL dialect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions\n",
    "\n",
    "We can use transactions too! For example, let's start a transaction and insert a new row to the test table we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬──────────────┬───────────────────┬────────────────────┬────────────────────┐\n",
       "│  id   │      timestamp      │     tags     │       col1        │        col3        │        col2        │\n",
       "│ int64 │       varchar       │  varchar[]   │      double       │       double       │       double       │\n",
       "├───────┼─────────────────────┼──────────────┼───────────────────┼────────────────────┼────────────────────┤\n",
       "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │              NULL │               NULL │               NULL │\n",
       "│     2 │ 2023-11-11T03:38:00 │ [d]          │ 143.2773325891486 │ 348.14722168023326 │ 241.33740756266872 │\n",
       "│   999 │ NULL                │ NULL         │              NULL │               NULL │               NULL │\n",
       "└───────┴─────────────────────┴──────────────┴───────────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "BEGIN;\n",
    "INSERT INTO pg.duckdbexamples.pg_test_table (id) VALUES (999);\n",
    "FROM pg.duckdbexamples.pg_test_table;\n",
    "\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ROLLBACK and the row is no longer there, as it shouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────────────────┬──────────────┬───────────────────┬────────────────────┬────────────────────┐\n",
       "│  id   │      timestamp      │     tags     │       col1        │        col3        │        col2        │\n",
       "│ int64 │       varchar       │  varchar[]   │      double       │       double       │       double       │\n",
       "├───────┼─────────────────────┼──────────────┼───────────────────┼────────────────────┼────────────────────┤\n",
       "│     1 │ 2023-04-16T20:12:06 │ [b, d, c, a] │              NULL │               NULL │               NULL │\n",
       "│     2 │ 2023-11-11T03:38:00 │ [d]          │ 143.2773325891486 │ 348.14722168023326 │ 241.33740756266872 │\n",
       "└───────┴─────────────────────┴──────────────┴───────────────────┴────────────────────┴────────────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "ROLLBACK;\n",
    "FROM pg.duckdbexamples.pg_test_table;\n",
    "\"\"\"\n",
    "duckdb.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For documentation regarding transactions in DuckDB, see [this](https://duckdb.org/docs/sql/statements/transactions.html), and for caveats with transactions when you have multiple databases attached, see [this](https://duckdb.org/docs/sql/statements/attach.html#transactional-semantics).\n",
    "\n",
    "Let's then delete the schema we created in Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"DROP SCHEMA pg.duckdbexamples CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Concurrency and Motherduck\n",
    "\n",
    "We saw above that we can create and use DuckDB databases with `duckdb.connect()`. An important thing to note with these databases is the [concurrency](https://duckdb.org/docs/connect/concurrency.html) model of DuckDB. In short:\n",
    "- a single process can read and write to a database, OR\n",
    "- multiple processes can only read a database.\n",
    "\n",
    "In other words, if a process needs to write to a database-file, no other process can already be accessing the database and no other process can access it while the connection is open. To illustrate this limitation, run the script `read_database.py` that simply tries to open the DuckDB database `test.db` we created above in read only mode. It will fail with an exception along the lines of\n",
    "```\n",
    "duckdb.duckdb.IOException: IO Error: Could not set lock on file \"/workspaces/DuckDB-examples/test.db\": Resource temporarily unavailable\n",
    "```\n",
    "To release the lock we currently have on the file we need to `DETACH`. Before we can `DETACH` the database, we need to switch to a different default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.sql(\"\"\"\n",
    "ATTACH ':memory:';\n",
    "USE memory;\n",
    "DETACH test;\n",
    "\"\"\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script `read_database.py` again. This time it should not throw any exceptions and it should keep the read only connection open until you terminate the script.\n",
    "\n",
    "Now that the database is locked in read only mode, we can not open it in write mode, but we can open it in read only mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in write mode failed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with duckdb.connect(\"test.db\") as write_conn:\n",
    "        write_conn.sql(\"SHOW TABLES\").show()\n",
    "        print(\"Managed to open in write mode.\")\n",
    "except duckdb.duckdb.IOException:\n",
    "    print(\"Opening in write mode failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────┐\n",
      "│       name        │\n",
      "│      varchar      │\n",
      "├───────────────────┤\n",
      "│ yet_another_table │\n",
      "└───────────────────┘\n",
      "\n",
      "Managed to open in read mode.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with duckdb.connect(\"test.db\", read_only=True) as read_conn:\n",
    "        read_conn.sql(\"SHOW TABLES\").show()\n",
    "        print(\"Managed to open in read mode.\")\n",
    "except duckdb.duckdb.IOException:\n",
    "    print(\"Opening in read mode failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two cells also show how you can use a context manager to handle the connection to a database file. If you use a context manager to open a connection to a database file, you then don't need to `DETACH` and `.close()` to release the lock and clean up the connection after you no longer need the connection. In general, it is good practice to use a context manager when you connect to database files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
